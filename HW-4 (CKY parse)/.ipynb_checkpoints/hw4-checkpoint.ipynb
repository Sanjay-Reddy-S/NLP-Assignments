{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: CKY Algorithm and Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: CKY Algorithm (30 points)\n",
    "\n",
    "In this section, you will implement the CKY algorithm for an unweighted CFG. See the starter code [cky.py](http://people.cs.umass.edu/~brenocon/inlp2017/hw4/cky.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.1 (8 points)\n",
    "Implement the acceptance version of CKY as ``cky_acceptance()``, which returns True if there is a ``S`` covering the entire sentence. Does it return True or False for the following sentences? Please ``pprint()`` the chart cells for each case as well. \n",
    "* the the\n",
    "* the table attacked a dog\n",
    "* the cat\n",
    "\n",
    "Hint: A simple way to implement the chart cells is by maintaining a list of nonterminals at the span. This list represents all possible nonterminals over that span. \n",
    "\n",
    "Hint: ``pprint()``ing the CKY chart cells may be useful for debugging.\n",
    "\n",
    "Hint: Python dictionaries allow tuples as keys. For example, ``d={}; d[(3,4)] = []``. A slight shortcut is that ``d[3,4]`` means the same thing as ``d[(3,4)]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar rules in tuple form:\n",
      "[('S', ('NP', 'VPp')),\n",
      " ('S', ('NPp', 'VPs')),\n",
      " ('S', ('NP', 'VP')),\n",
      " ('NP', ('Det', 'Noun')),\n",
      " ('NPp', ('Det', 'NounP')),\n",
      " ('VP', ('Verb', 'NP')),\n",
      " ('VPs', ('VerbS', 'NP')),\n",
      " ('VPp', ('VerbP', 'NP')),\n",
      " ('PP', ('Prep', 'NP')),\n",
      " ('NP', ('NP', 'PP')),\n",
      " ('VP', ('VP', 'PP'))]\n",
      "Rule parents indexed by children:\n",
      "{('Det', 'Noun'): ['NP'],\n",
      " ('Det', 'NounP'): ['NPp'],\n",
      " ('NP', 'PP'): ['NP'],\n",
      " ('NP', 'VP'): ['S'],\n",
      " ('NP', 'VPp'): ['S'],\n",
      " ('NPp', 'VPs'): ['S'],\n",
      " ('Prep', 'NP'): ['PP'],\n",
      " ('VP', 'PP'): ['VP'],\n",
      " ('Verb', 'NP'): ['VP'],\n",
      " ('VerbP', 'NP'): ['VPp'],\n",
      " ('VerbS', 'NP'): ['VPs']}\n",
      "Lexicon:\n",
      "{'Det': set(['a', 'the']),\n",
      " 'Noun': set(['cat', 'dog', 'food', 'table']),\n",
      " 'NounP': set(['cats', 'dogs']),\n",
      " 'Prep': set(['in', 'of', 'on', 'with']),\n",
      " 'Verb': set(['attacked', 'hated', 'loved', 'saw']),\n",
      " 'VerbP': set(['attacks']),\n",
      " 'VerbS': set(['attack'])}\n"
     ]
    }
   ],
   "source": [
    "import cky\n",
    "from pprint import pprint\n",
    "print \"Grammar rules in tuple form:\"\n",
    "pprint(cky.grammar_rules)\n",
    "print \"Rule parents indexed by children:\"\n",
    "pprint(cky.possible_parents_for_children)\n",
    "print \"Lexicon:\"\n",
    "pprint(cky.lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the\n",
      "{(0, 1): ['Det'], (0, 2): [], (1, 2): ['Det']}\n",
      "False\n",
      "**************************************************\n",
      "the table attacked a dog\n",
      "{(0, 1): ['Det'],\n",
      " (0, 2): ['NP'],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): ['S'],\n",
      " (1, 2): ['Noun'],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (2, 3): ['Verb'],\n",
      " (2, 4): [],\n",
      " (2, 5): ['VP'],\n",
      " (3, 4): ['Det'],\n",
      " (3, 5): ['NP'],\n",
      " (4, 5): ['Noun']}\n",
      "True\n",
      "**************************************************\n",
      "the cat\n",
      "{(0, 1): ['Det'], (0, 2): ['NP'], (1, 2): ['Noun']}\n",
      "False\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "#Print the result here\n",
    "import cky;reload(cky)\n",
    "print \"the the\"\n",
    "print cky.cky_acceptance([\"the\", \"the\"])\n",
    "print '*'*50\n",
    "print \"the table attacked a dog\"\n",
    "print cky.cky_acceptance([\"the\", \"table\", \"attacked\", \"a\", \"dog\"])\n",
    "print \"*\" * 50\n",
    "print \"the cat\"\n",
    "print cky.cky_acceptance([\"the\", \"cat\"])\n",
    "print \"*\" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Question 1.2 (15 points)\n",
    "Implement the parsing version of CKY, which returns one of the legal parses for the sentence (and returns None if there are none). If there are multiple real parses, we don’t care which one you print. Implement this as `cky_parse()`. You probably want to start by copying your `cky_acceptance()` answer and modifying it. Have it return the parse in the following format, using nested lists to represent the tree (this is a simple Python variant of the Lisp-style S-expression that’s usually used for this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "['S',\n",
    "        [['NP', [['Det', 'the'], ['Noun', 'cat']]],\n",
    "         ['VP', [['Verb', 'attacked'], \n",
    "                 ['NP', [['Det', 'the'], ['Noun', 'food']]]]]]]\n",
    "                 ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the parses for the following sentences.  \n",
    "* the cat saw a dog\n",
    "* the cat saw a dog in a table\n",
    "* the cat with a table attacked the food  \n",
    "\n",
    "Hint: In the chart cells, you will now have to store backpointers as well. One way to do it is to store a list of tuples, each of which is  ``(nonterminal, splitpoint, leftchild nonterm, rightchild nonterm)``. For example, if the state ``('NP', 3, 'Det', 'Noun')`` is in the cell for span (2,4), that means this is a chart state of symbol NP, which came from a ``Det`` at position (2,3) and a Noun at position (3,4).\n",
    "\n",
    "Hint: It may be useful to use a recursive function for the backtrace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('S', 2, 'NP', 'VP')],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (2, 3): [('Verb', 0, '', 'saw')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('VP', 3, 'Verb', 'NP')],\n",
      " (3, 4): [('Det', 0, '', 'a')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (4, 5): [('Noun', 0, '', 'dog')]}\n",
      "['S',\n",
      " ['NP', ['Det', 'the'], ['Noun', 'cat']],\n",
      " ['VP', ['Verb', 'saw'], ['NP', ['Det', 'a'], ['Noun', 'dog']]]]\n",
      "**************************************************\n",
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('S', 2, 'NP', 'VP')],\n",
      " (0, 6): [],\n",
      " (0, 7): [],\n",
      " (0, 8): [('S', 2, 'NP', 'VP')],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (1, 6): [],\n",
      " (1, 7): [],\n",
      " (1, 8): [],\n",
      " (2, 3): [('Verb', 0, '', 'saw')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('VP', 3, 'Verb', 'NP')],\n",
      " (2, 6): [],\n",
      " (2, 7): [],\n",
      " (2, 8): [('VP', 3, 'Verb', 'NP'), ('VP', 5, 'VP', 'PP')],\n",
      " (3, 4): [('Det', 0, '', 'a')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (3, 6): [],\n",
      " (3, 7): [],\n",
      " (3, 8): [('NP', 5, 'NP', 'PP')],\n",
      " (4, 5): [('Noun', 0, '', 'dog')],\n",
      " (4, 6): [],\n",
      " (4, 7): [],\n",
      " (4, 8): [],\n",
      " (5, 6): [('Prep', 0, '', 'in')],\n",
      " (5, 7): [],\n",
      " (5, 8): [('PP', 6, 'Prep', 'NP')],\n",
      " (6, 7): [('Det', 0, '', 'a')],\n",
      " (6, 8): [('NP', 7, 'Det', 'Noun')],\n",
      " (7, 8): [('Noun', 0, '', 'table')]}\n",
      "['S',\n",
      " ['NP', ['Det', 'the'], ['Noun', 'cat']],\n",
      " ['VP',\n",
      "  ['Verb', 'saw'],\n",
      "  ['NP',\n",
      "   ['NP', ['Det', 'a'], ['Noun', 'dog']],\n",
      "   ['PP', ['Prep', 'in'], ['NP', ['Det', 'a'], ['Noun', 'table']]]]]]\n",
      "**************************************************\n",
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('NP', 2, 'NP', 'PP')],\n",
      " (0, 6): [],\n",
      " (0, 7): [],\n",
      " (0, 8): [('S', 5, 'NP', 'VP')],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (1, 6): [],\n",
      " (1, 7): [],\n",
      " (1, 8): [],\n",
      " (2, 3): [('Prep', 0, '', 'with')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('PP', 3, 'Prep', 'NP')],\n",
      " (2, 6): [],\n",
      " (2, 7): [],\n",
      " (2, 8): [],\n",
      " (3, 4): [('Det', 0, '', 'a')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (3, 6): [],\n",
      " (3, 7): [],\n",
      " (3, 8): [('S', 5, 'NP', 'VP')],\n",
      " (4, 5): [('Noun', 0, '', 'table')],\n",
      " (4, 6): [],\n",
      " (4, 7): [],\n",
      " (4, 8): [],\n",
      " (5, 6): [('Verb', 0, '', 'attacked')],\n",
      " (5, 7): [],\n",
      " (5, 8): [('VP', 6, 'Verb', 'NP')],\n",
      " (6, 7): [('Det', 0, '', 'the')],\n",
      " (6, 8): [('NP', 7, 'Det', 'Noun')],\n",
      " (7, 8): [('Noun', 0, '', 'food')]}\n",
      "['S',\n",
      " ['NP',\n",
      "  ['NP', ['Det', 'the'], ['Noun', 'cat']],\n",
      "  ['PP', ['Prep', 'with'], ['NP', ['Det', 'a'], ['Noun', 'table']]]],\n",
      " ['VP', ['Verb', 'attacked'], ['NP', ['Det', 'the'], ['Noun', 'food']]]]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Output the results for each sentence.\n",
    "#TODO: Print out the parse tree for each of the three sentence\n",
    "pprint(cky.cky_parse(['the','cat','saw','a','dog']))\n",
    "print \"*\" * 50\n",
    "pprint(cky.cky_parse(['the','cat','saw','a','dog','in','a','table']))\n",
    "print \"*\" * 50\n",
    "pprint( cky.cky_parse(['the','cat','with','a','table','attacked','the','food']) )\n",
    "print \"*\" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.3 (7 points)\n",
    "Revise the grammar as follows.\n",
    "\n",
    "* Add four words to the lexicon: two verbs “attack” and “attacks”, and the nouns “cats” and “dogs”.\n",
    "* Revise the rules to enforce subject-verb agreement on number.\n",
    "\n",
    "The new grammar should accept and reject the following sentences. Please run your parser on these sentences and report the parse trees for the accepted ones. Also, describe how you changed the grammar, and why.\n",
    "\n",
    "ACCEPT: ``the cat attacks the dog``   \n",
    "REJECT: ``the cat attack the dog``  \n",
    "ACCEPT: ``the cats attack the dog``  \n",
    "REJECT: ``the cat with the food on a dog attack the dog``\n",
    "\n",
    "Hint: you will need to introduce new nonterminal symbols, and modify the currently existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('S', 2, 'NP', 'VPp')],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (2, 3): [('VerbP', 0, '', 'attacks')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('VPp', 3, 'VerbP', 'NP')],\n",
      " (3, 4): [('Det', 0, '', 'the')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (4, 5): [('Noun', 0, '', 'dog')]}\n",
      "['S',\n",
      " ['NP', ['Det', 'the'], ['Noun', 'cat']],\n",
      " ['VPp', ['VerbP', 'attacks'], ['NP', ['Det', 'the'], ['Noun', 'dog']]]]\n",
      "**************************************************\n",
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (2, 3): [('VerbS', 0, '', 'attack')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('VPs', 3, 'VerbS', 'NP')],\n",
      " (3, 4): [('Det', 0, '', 'the')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (4, 5): [('Noun', 0, '', 'dog')]}\n",
      "None\n",
      "**************************************************\n",
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NPp', 1, 'Det', 'NounP')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('S', 2, 'NPp', 'VPs')],\n",
      " (1, 2): [('NounP', 0, '', 'cats')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (2, 3): [('VerbS', 0, '', 'attack')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('VPs', 3, 'VerbS', 'NP')],\n",
      " (3, 4): [('Det', 0, '', 'the')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (4, 5): [('Noun', 0, '', 'dog')]}\n",
      "['S',\n",
      " ['NPp', ['Det', 'the'], ['NounP', 'cats']],\n",
      " ['VPs', ['VerbS', 'attack'], ['NP', ['Det', 'the'], ['Noun', 'dog']]]]\n",
      "**************************************************\n",
      "{(0, 1): [('Det', 0, '', 'the')],\n",
      " (0, 2): [('NP', 1, 'Det', 'Noun')],\n",
      " (0, 3): [],\n",
      " (0, 4): [],\n",
      " (0, 5): [('NP', 2, 'NP', 'PP')],\n",
      " (0, 6): [],\n",
      " (0, 7): [],\n",
      " (0, 8): [('NP', 2, 'NP', 'PP'), ('NP', 5, 'NP', 'PP')],\n",
      " (0, 9): [],\n",
      " (0, 10): [],\n",
      " (0, 11): [],\n",
      " (1, 2): [('Noun', 0, '', 'cat')],\n",
      " (1, 3): [],\n",
      " (1, 4): [],\n",
      " (1, 5): [],\n",
      " (1, 6): [],\n",
      " (1, 7): [],\n",
      " (1, 8): [],\n",
      " (1, 9): [],\n",
      " (1, 10): [],\n",
      " (1, 11): [],\n",
      " (2, 3): [('Prep', 0, '', 'with')],\n",
      " (2, 4): [],\n",
      " (2, 5): [('PP', 3, 'Prep', 'NP')],\n",
      " (2, 6): [],\n",
      " (2, 7): [],\n",
      " (2, 8): [('PP', 3, 'Prep', 'NP')],\n",
      " (2, 9): [],\n",
      " (2, 10): [],\n",
      " (2, 11): [],\n",
      " (3, 4): [('Det', 0, '', 'the')],\n",
      " (3, 5): [('NP', 4, 'Det', 'Noun')],\n",
      " (3, 6): [],\n",
      " (3, 7): [],\n",
      " (3, 8): [('NP', 5, 'NP', 'PP')],\n",
      " (3, 9): [],\n",
      " (3, 10): [],\n",
      " (3, 11): [],\n",
      " (4, 5): [('Noun', 0, '', 'food')],\n",
      " (4, 6): [],\n",
      " (4, 7): [],\n",
      " (4, 8): [],\n",
      " (4, 9): [],\n",
      " (4, 10): [],\n",
      " (4, 11): [],\n",
      " (5, 6): [('Prep', 0, '', 'on')],\n",
      " (5, 7): [],\n",
      " (5, 8): [('PP', 6, 'Prep', 'NP')],\n",
      " (5, 9): [],\n",
      " (5, 10): [],\n",
      " (5, 11): [],\n",
      " (6, 7): [('Det', 0, '', 'a')],\n",
      " (6, 8): [('NP', 7, 'Det', 'Noun')],\n",
      " (6, 9): [],\n",
      " (6, 10): [],\n",
      " (6, 11): [],\n",
      " (7, 8): [('Noun', 0, '', 'dog')],\n",
      " (7, 9): [],\n",
      " (7, 10): [],\n",
      " (7, 11): [],\n",
      " (8, 9): [('VerbS', 0, '', 'attack')],\n",
      " (8, 10): [],\n",
      " (8, 11): [('VPs', 9, 'VerbS', 'NP')],\n",
      " (9, 10): [('Det', 0, '', 'the')],\n",
      " (9, 11): [('NP', 10, 'Det', 'Noun')],\n",
      " (10, 11): [('Noun', 0, '', 'dog')]}\n",
      "None\n",
      "**************************************************\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Output the results for each sentence.\n",
    "#TODO: Print out the parse tree for each of the four sentence\n",
    "pprint( cky.cky_parse(['the','cat','attacks','the','dog']) )\n",
    "print \"*\" * 50\n",
    "pprint( cky.cky_parse(['the','cat','attack','the','dog']) )\n",
    "print \"*\" * 50\n",
    "pprint( cky.cky_parse(['the','cats','attack','the','dog']) )\n",
    "print \"*\" * 50\n",
    "pprint( cky.cky_parse(['the','cat','with','the','food','on','a','dog','attack','the','dog']))\n",
    "print \"*\" * 50\n",
    "print '*'*50\n",
    "#print \"Just for my experimentation\"\n",
    "#pprint(cky.cky_parse(['the','cat','with','the','food','on','a','dog','attacks','the','dog']))\n",
    "#print (\"The above one prints Parse tree as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Weighted CKY Algorithm (40 points)\n",
    "In this section you will implement the weighted CKY Algorithm for a Probabilistic CFG. You will have to make modifications to the existing algorithm to account for the probabilities and your parse function should output the most probable parse tree. \n",
    "Please write all your code in ``weighted_cky.py`` file for this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.1 (7 points)\n",
    "The CKY Algorithm requires the CFG to be in Chomsky Normal Form. Convert the following CFG into Chomsky Normal Form. (For the sake of uniformity, replace the leftmost pairs of non-terminals with new non-terminal)\n",
    "\n",
    "S -> Aux NP VP   \n",
    "S -> VP  \n",
    "VP -> Verb NP  \n",
    "VP -> VP PP  \n",
    "Verb -> book  \n",
    "Aux -> does  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n",
    "1)S   ->A1   VP\n",
    "2)A1  ->Aux  NP\n",
    "\n",
    "3)S   ->Verb NP\n",
    "4)S   ->VP   PP\n",
    "\n",
    "5)Verb->book\n",
    "6)Aux ->does\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.2 (8 points)\n",
    "We will now implement a weighted CYK algorithm to parse a sentence and return the most probable parse tree. \n",
    "The grammar is defined in ``pcfg_grammar_original.txt``. As you can notice, some of the rules are not in CNF. \n",
    "Modify the ``pcfg_grammar_modified.txt`` file such that all the rules are in Chomsky Normal Form.\n",
    "(For the sake of uniformity, replace the leftmost pairs of non-terminals with new non-terminal)\n",
    "\n",
    "Note: When transforming the grammar to CNF, must set production probabilities to preserve the probability of derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 2.3 (5 points)\n",
    "Explain briefly what are the changes you made to convert the grammar into CNF Form. Why did you make these changes?\n",
    "\n",
    "Grammars in CNF are restricted to rules of the form A→BC or A→w. (Right hand-side is always two non-terminals or a single terminal). This naturally lends to a binary tree form which is very crucial to CYK algo (we look at 'TWO' cells, before making a decision in the algorithm). Normal forms (like CNF) can lend us more structure to work with, which in turn results in easier parsing algorithms. (One interesting point is: number of derivations to derive any string of length ’n’ from CNF grammer is always 2n + 1, because at worst case, each point has a two-way split. Such interesting things can be gathered only because it is CNF). In normal Context Free Grammar, 'e'psilon transitions can exist which make designing polynomial time algorithms very difficult (we have to back-track in such a case dealing with generic CFG).\n",
    "\n",
    "1) When non-terminals and terminals are mixed... we simply replace the terminal with a dummy non-terminal. \n",
    "\n",
    "2) When there are more than 2 non-terminals on RHS also, we introduce a dummy non-terminal and start grouping (from left)\n",
    "\n",
    "3) When there is a single non-terminal like A->B, we traverse forward and replace B with non-terminals/terminals they lead to.\n",
    "\n",
    "Probability also needs to be appropriately modified (Sum should be 1. And when a rule is broken down... the product of constituents should still be same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.4 (8 points)\n",
    "Complete the ``populate_grammar_rules()`` function in the ``weighted_cky.py`` script. This function will have to read in the grammar rules from ``pcfg_grammar_modified.txt`` file and populate the `grammar_rules` and `lexicon` data structure. Additionally you would need to store the probability mapping in a suitable data structure. \n",
    "\n",
    "Hint: You can modify the starter code provided in cky.py for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar rules in tuple form:\n",
      "[('S', ('NP', 'VP')),\n",
      " ('S', ('A1', 'VP')),\n",
      " ('A1', ('Aux', 'NP')),\n",
      " ('S', ('Verb', 'NP')),\n",
      " ('S', ('VP', 'PP')),\n",
      " ('NP', ('Det', 'Nominal')),\n",
      " ('Nominal', ('Nominal', 'Noun')),\n",
      " ('Nominal', ('Nominal', 'PP')),\n",
      " ('VP', ('Verb', 'NP')),\n",
      " ('VP', ('VP', 'PP')),\n",
      " ('PP', ('Prep', 'NP'))]\n",
      "Rule parents indexed by children:\n",
      "{('A1', 'VP'): ['S'],\n",
      " ('Aux', 'NP'): ['A1'],\n",
      " ('Det', 'Nominal'): ['NP'],\n",
      " ('NP', 'VP'): ['S'],\n",
      " ('Nominal', 'Noun'): ['Nominal'],\n",
      " ('Nominal', 'PP'): ['Nominal'],\n",
      " ('Prep', 'NP'): ['PP'],\n",
      " ('VP', 'PP'): ['S', 'VP'],\n",
      " ('Verb', 'NP'): ['S', 'VP']}\n",
      "probabilities\n",
      "{('A1', ('Aux', 'NP')): 1.0,\n",
      " ('Aux', ('does',)): 1.0,\n",
      " ('Det', ('a',)): 0.2,\n",
      " ('Det', ('that',)): 0.1,\n",
      " ('Det', ('the',)): 0.6,\n",
      " ('Det', ('this',)): 0.1,\n",
      " ('NP', ('Det', 'Nominal')): 0.6,\n",
      " ('NP', ('Houston',)): 0.16,\n",
      " ('NP', ('I',)): 0.1,\n",
      " ('NP', ('NWA',)): 0.04,\n",
      " ('NP', ('he',)): 0.02,\n",
      " ('NP', ('me',)): 0.06,\n",
      " ('NP', ('she',)): 0.02,\n",
      " ('Nominal', ('Nominal', 'Noun')): 0.2,\n",
      " ('Nominal', ('Nominal', 'PP')): 0.5,\n",
      " ('Nominal', ('book',)): 0.03,\n",
      " ('Nominal', ('flight',)): 0.15,\n",
      " ('Nominal', ('meal',)): 0.06,\n",
      " ('Nominal', ('money',)): 0.06,\n",
      " ('Noun', ('book',)): 0.1,\n",
      " ('Noun', ('flight',)): 0.5,\n",
      " ('Noun', ('meal',)): 0.2,\n",
      " ('Noun', ('money',)): 0.2,\n",
      " ('PP', ('Prep', 'NP')): 1.0,\n",
      " ('Prep', ('from',)): 0.25,\n",
      " ('Prep', ('near',)): 0.2,\n",
      " ('Prep', ('on',)): 0.1,\n",
      " ('Prep', ('through',)): 0.2,\n",
      " ('Prep', ('to',)): 0.25,\n",
      " ('Pronoun', ('I',)): 0.5,\n",
      " ('Pronoun', ('he',)): 0.1,\n",
      " ('Pronoun', ('me',)): 0.3,\n",
      " ('Pronoun', ('she',)): 0.1,\n",
      " ('Proper-Noun', ('Houston',)): 0.8,\n",
      " ('Proper-Noun', ('NWA',)): 0.2,\n",
      " ('S', ('A1', 'VP')): 0.1,\n",
      " ('S', ('NP', 'VP')): 0.8,\n",
      " ('S', ('VP', 'PP')): 0.03,\n",
      " ('S', ('Verb', 'NP')): 0.05,\n",
      " ('S', ('book',)): 0.01,\n",
      " ('S', ('include',)): 0.004,\n",
      " ('S', ('prefer',)): 0.006,\n",
      " ('VP', ('VP', 'PP')): 0.3,\n",
      " ('VP', ('Verb', 'NP')): 0.5,\n",
      " ('VP', ('book',)): 0.1,\n",
      " ('VP', ('include',)): 0.04,\n",
      " ('VP', ('prefer',)): 0.06,\n",
      " ('Verb', ('book',)): 0.5,\n",
      " ('Verb', ('include',)): 0.2,\n",
      " ('Verb', ('prefer',)): 0.3}\n",
      "Lexicon\n",
      "{'Aux': set(['does']),\n",
      " 'Det': set(['a', 'that', 'the', 'this']),\n",
      " 'NP': set(['Houston', 'I', 'NWA', 'he', 'me', 'she']),\n",
      " 'Nominal': set(['book', 'flight', 'meal', 'money']),\n",
      " 'Noun': set(['book', 'flight', 'meal', 'money']),\n",
      " 'Prep': set(['from', 'near', 'on', 'through', 'to']),\n",
      " 'Pronoun': set(['I', 'he', 'me', 'she']),\n",
      " 'Proper-Noun': set(['Houston', 'NWA']),\n",
      " 'S': set(['book', 'include', 'prefer']),\n",
      " 'VP': set(['book', 'include', 'prefer']),\n",
      " 'Verb': set(['book', 'include', 'prefer'])}\n",
      "Grammar rules in tuple form:\n",
      "[('S', ('NP', 'VP')),\n",
      " ('S', ('A1', 'VP')),\n",
      " ('A1', ('Aux', 'NP')),\n",
      " ('S', ('Verb', 'NP')),\n",
      " ('S', ('VP', 'PP')),\n",
      " ('NP', ('Det', 'Nominal')),\n",
      " ('Nominal', ('Nominal', 'Noun')),\n",
      " ('Nominal', ('Nominal', 'PP')),\n",
      " ('VP', ('Verb', 'NP')),\n",
      " ('VP', ('VP', 'PP')),\n",
      " ('PP', ('Prep', 'NP')),\n",
      " ('S', ('NP', 'VP')),\n",
      " ('S', ('A1', 'VP')),\n",
      " ('A1', ('Aux', 'NP')),\n",
      " ('S', ('Verb', 'NP')),\n",
      " ('S', ('VP', 'PP')),\n",
      " ('NP', ('Det', 'Nominal')),\n",
      " ('Nominal', ('Nominal', 'Noun')),\n",
      " ('Nominal', ('Nominal', 'PP')),\n",
      " ('VP', ('Verb', 'NP')),\n",
      " ('VP', ('VP', 'PP')),\n",
      " ('PP', ('Prep', 'NP'))]\n",
      "Rule parents indexed by children:\n",
      "{('A1', 'VP'): ['S', 'S', 'S'],\n",
      " ('Aux', 'NP'): ['A1', 'A1', 'A1'],\n",
      " ('Det', 'Nominal'): ['NP', 'NP', 'NP'],\n",
      " ('NP', 'VP'): ['S', 'S', 'S'],\n",
      " ('Nominal', 'Noun'): ['Nominal', 'Nominal', 'Nominal'],\n",
      " ('Nominal', 'PP'): ['Nominal', 'Nominal', 'Nominal'],\n",
      " ('Prep', 'NP'): ['PP', 'PP', 'PP'],\n",
      " ('VP', 'PP'): ['S', 'VP', 'S', 'VP', 'S', 'VP'],\n",
      " ('Verb', 'NP'): ['S', 'VP', 'S', 'VP', 'S', 'VP']}\n",
      "probabilities\n",
      "{('A1', ('Aux', 'NP')): 1.0,\n",
      " ('Aux', ('does',)): 1.0,\n",
      " ('Det', ('a',)): 0.2,\n",
      " ('Det', ('that',)): 0.1,\n",
      " ('Det', ('the',)): 0.6,\n",
      " ('Det', ('this',)): 0.1,\n",
      " ('NP', ('Det', 'Nominal')): 0.6,\n",
      " ('NP', ('Houston',)): 0.16,\n",
      " ('NP', ('I',)): 0.1,\n",
      " ('NP', ('NWA',)): 0.04,\n",
      " ('NP', ('he',)): 0.02,\n",
      " ('NP', ('me',)): 0.06,\n",
      " ('NP', ('she',)): 0.02,\n",
      " ('Nominal', ('Nominal', 'Noun')): 0.2,\n",
      " ('Nominal', ('Nominal', 'PP')): 0.5,\n",
      " ('Nominal', ('book',)): 0.03,\n",
      " ('Nominal', ('flight',)): 0.15,\n",
      " ('Nominal', ('meal',)): 0.06,\n",
      " ('Nominal', ('money',)): 0.06,\n",
      " ('Noun', ('book',)): 0.1,\n",
      " ('Noun', ('flight',)): 0.5,\n",
      " ('Noun', ('meal',)): 0.2,\n",
      " ('Noun', ('money',)): 0.2,\n",
      " ('PP', ('Prep', 'NP')): 1.0,\n",
      " ('Prep', ('from',)): 0.25,\n",
      " ('Prep', ('near',)): 0.2,\n",
      " ('Prep', ('on',)): 0.1,\n",
      " ('Prep', ('through',)): 0.2,\n",
      " ('Prep', ('to',)): 0.25,\n",
      " ('Pronoun', ('I',)): 0.5,\n",
      " ('Pronoun', ('he',)): 0.1,\n",
      " ('Pronoun', ('me',)): 0.3,\n",
      " ('Pronoun', ('she',)): 0.1,\n",
      " ('Proper-Noun', ('Houston',)): 0.8,\n",
      " ('Proper-Noun', ('NWA',)): 0.2,\n",
      " ('S', ('A1', 'VP')): 0.1,\n",
      " ('S', ('NP', 'VP')): 0.8,\n",
      " ('S', ('VP', 'PP')): 0.03,\n",
      " ('S', ('Verb', 'NP')): 0.05,\n",
      " ('S', ('book',)): 0.01,\n",
      " ('S', ('include',)): 0.004,\n",
      " ('S', ('prefer',)): 0.006,\n",
      " ('VP', ('VP', 'PP')): 0.3,\n",
      " ('VP', ('Verb', 'NP')): 0.5,\n",
      " ('VP', ('book',)): 0.1,\n",
      " ('VP', ('include',)): 0.04,\n",
      " ('VP', ('prefer',)): 0.06,\n",
      " ('Verb', ('book',)): 0.5,\n",
      " ('Verb', ('include',)): 0.2,\n",
      " ('Verb', ('prefer',)): 0.3}\n",
      "Lexicon\n",
      "{'Aux': set(['does']),\n",
      " 'Det': set(['a', 'that', 'the', 'this']),\n",
      " 'NP': set(['Houston', 'I', 'NWA', 'he', 'me', 'she']),\n",
      " 'Nominal': set(['book', 'flight', 'meal', 'money']),\n",
      " 'Noun': set(['book', 'flight', 'meal', 'money']),\n",
      " 'Prep': set(['from', 'near', 'on', 'through', 'to']),\n",
      " 'Pronoun': set(['I', 'he', 'me', 'she']),\n",
      " 'Proper-Noun': set(['Houston', 'NWA']),\n",
      " 'S': set(['book', 'include', 'prefer']),\n",
      " 'VP': set(['book', 'include', 'prefer']),\n",
      " 'Verb': set(['book', 'include', 'prefer'])}\n"
     ]
    }
   ],
   "source": [
    "from weighted_cky import populate_grammar_rules\n",
    "populate_grammar_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.5 (12 points)\n",
    "Implement the weighted parsing version of CKY, which returns the most probable legal parse for the sentence (and returns None if there are none). If there are multiple real parses, this function will always return the most probable parse i.e the one with maximum probability. \n",
    "Complete the ``pcky_parse()``.\n",
    "Print the parse tree and the probabilities for the following sentences:\n",
    "* book the flight through Houston\n",
    "* include this book\n",
    "* the the\n",
    "\n",
    "Hint: You can use the code in `cky_parse()` and modify it to accomodate probabilities and compute the most probable parse.   \n",
    "Note: The topmost cell should contain rules associated with the `S` non terminal, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'PP', 'Noun', 'Nominal', 'Pronoun', 'Det', 'VP', 'Proper-Noun', 'S', 'Verb', 'NP', 'Aux', 'Prep']\n",
      "{(0, 1): [0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0],\n",
      " (0, 2): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (1, 2): [0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0]}\n",
      "False\n",
      "**************************************************\n",
      "['A1', 'PP', 'Noun', 'Nominal', 'Pronoun', 'Det', 'VP', 'Proper-Noun', 'S', 'Verb', 'NP', 'Aux', 'Prep']\n",
      "{(0, 1): [0, 0, 0, 0, 0, 0, 0.04, 0, 0.004, 0.2, 0, 0, 0],\n",
      " (0, 2): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (0, 3): [0, 0, 0, 0, 0, 0, 0.00018, 0, 1.8000000000000004e-05, 0, 0, 0, 0],\n",
      " (1, 2): [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0],\n",
      " (1, 3): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0018, 0, 0],\n",
      " (2, 3): [0, 0, 0.1, 0.03, 0, 0, 0.1, 0, 0.01, 0.5, 0, 0, 0]}\n",
      "Probability is: 1.8e-05\n",
      "['S', ['Verb', 'include'], ['NP', ['Det', 'this'], ['Nominal', 'book']]]\n",
      "**************************************************\n",
      "['A1', 'PP', 'Noun', 'Nominal', 'Pronoun', 'Det', 'VP', 'Proper-Noun', 'S', 'Verb', 'NP', 'Aux', 'Prep']\n",
      "{(0, 1): [0, 0, 0.1, 0.03, 0, 0, 0.1, 0, 0.01, 0.5, 0, 0, 0],\n",
      " (0, 2): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (0, 3): [0, 0, 0, 0, 0, 0, 0.0135, 0, 0.00135, 0, 0, 0, 0],\n",
      " (0, 4): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (0, 5): [0,\n",
      "          0,\n",
      "          0,\n",
      "          0,\n",
      "          0,\n",
      "          0,\n",
      "          0.00021599999999999996,\n",
      "          0,\n",
      "          2.1599999999999996e-05,\n",
      "          0,\n",
      "          0,\n",
      "          0,\n",
      "          0],\n",
      " (1, 2): [0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0, 0],\n",
      " (1, 3): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.054, 0, 0],\n",
      " (1, 4): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (1, 5): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0008639999999999999, 0, 0],\n",
      " (2, 3): [0, 0, 0.5, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (2, 4): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (2, 5): [0, 0, 0, 0.0024, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (3, 4): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2],\n",
      " (3, 5): [0, 0.032, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " (4, 5): [0, 0, 0, 0, 0, 0, 0, 0.8, 0, 0, 0.16, 0, 0]}\n",
      "Probability is: 2.16e-05\n",
      "['S',\n",
      " ['Verb', 'book'],\n",
      " ['NP',\n",
      "  ['Det', 'the'],\n",
      "  ['Nominal',\n",
      "   ['Nominal', 'flight'],\n",
      "   ['PP', ['Prep', 'through'], ['NP', 'Houston']]]]]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from weighted_cky import pcky_parse\n",
    "# Output the results for each sentence.\n",
    "#TODO: Print out the parse tree for each of the three sentence\n",
    "pprint( pcky_parse(['the','the']) )\n",
    "print \"*\" * 50\n",
    "pprint (pcky_parse(['include' ,'this', 'book']))\n",
    "print \"*\" * 50\n",
    "pprint(pcky_parse(['book','the', 'flight', 'through', 'Houston']))\n",
    "print \"*\" * 50\n",
    "#pprint(pcky_parse(['the', 'the']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Dependency parser output (30 points)\n",
    "\n",
    "You will conduct manual error analysis of [CoreNLP](https://stanfordnlp.github.io/CoreNLP/)'s dependency parser.\n",
    "\n",
    "Create an English sentence where the parser makes an error, and you know what the correct analysis ought to be, according to the Universal Dependencies grammatical standard.  You will want to play around with different sentences, look at their output, and check against the Universal Dependencies annotation standard.  The current version of CoreNLP outputs according to the \"UD version 1\" standard, so please use this page:\n",
    " * [UD v1 homepage](http://universaldependencies.org/docsv1/)\n",
    " * and in particular, the [UD v1 dependency relations list](http://universaldependencies.org/docsv1/u/dep/index.html)\n",
    "\n",
    "For quickly looking at things, their [online demo](http://corenlp.run/) may be useful.\n",
    "\n",
    "However, for this assignment, you need to run the parser to output in \"conllu\" format, which is human readable.  You need to download and run the parser for this.  (It requires Java.) Use version 3.8.0 (it should be the current version). You can it working in interactive mode so you can just type sentences into it on the terminal like this:\n",
    "\n",
    "```\n",
    "./corenlp.sh -annotators tokenize,ssplit,pos,lemma,depparse -outputFormat conllu \n",
    "[...]\n",
    "Entering interactive shell. Type q RETURN or EOF to quit.\n",
    "NLP> \n",
    "```\n",
    "\n",
    "For example then you can type\n",
    "```\n",
    "NLP> I saw a cat.\n",
    "1       I       I       _       PRP     _       2       nsubj   _       _\n",
    "2       saw     see     _       VBD     _       0       root    _       _\n",
    "3       a       a       _       DT      _       4       det     _       _\n",
    "4       cat     cat     _       NN      _       2       dobj    _       _\n",
    "5       .       .       _       .       _       2       punct   _       _\n",
    "```\n",
    "\n",
    "You can also use the `-inputFile` flag if you'd rather give it a whole file at once.\n",
    "\n",
    "As you can see in the parser documentation, the 7th and 8th columns describe the dependency edge for the word's parent (a.k.a governor): it has the index of its parent, and the edge label (a.k.a. the relation).  For example, this parse contains the dependency edge *nsubj(saw:2, I:1)* meaning that \"I\" is the subject of \"saw\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Once you've decided your sentence, please put the conllu-formatted parser output below in the markdown triple-quoted area.  Please be very careful where it goes since we will use a script to pull it out.\n",
    "    \n",
    "PARSE GOES BELOW HERE\n",
    "```\n",
    "1\tYou\tyou\t_\tPRP\t_\t3\tnsubj\t_\t_\n",
    "2\tshould\tshould\t_\tMD\t_\t3\taux\t_\t_\n",
    "3\tcheck\tcheck\t_\tVB\t_\t0\troot\t_\t_\n",
    "4\tthe\tthe\t_\tDT\t_\t5\tdet\t_\t_\n",
    "5\tmileage\tmileage\t_\tNN\t_\t3\tdobj\t_\t_\n",
    "6\ton\ton\t_\tIN\t_\t9\tcase\t_\t_\n",
    "7\tyour\tyou\t_\tPRP$\t_\t9\tnmod:poss\t_\t_\n",
    "8\tred\tred\t_\tJJ\t_\t9\tamod\t_\t_\n",
    "9\tcar\tcar\t_\tNN\t_\t3\tnmod\t_\t_\n",
    "10\t.\t.\t_\t.\t_\t3\tpunct\t_\t_\n",
    "\n",
    "```\n",
    "PARSE GOES ABOVE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** Please describe the error you found.  Also give a citation and link to the relevant part of the UD documentation describing one of the relations that the parser predicted in error, or did something wrong with.\n",
    "\n",
    "There is no dependency link between car and mileage (Mileage is a property of car). In fact, by removing the adjective 'red' and using the sentence: \"You should check the mileage on your car\" gives the correct link (nmod) between mileage and car. The [nmod](http://universaldependencies.org/docsv1/u/dep/nmod.html) between two nouns corresponds to an attribute (here property of the noun)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question 3.3:** Please give correct that error in the parse .  Put your corrected parse, again in that conllu textual format, below.  You should take a copy of the output and manually change some of the 7th/8th dependency edge columns.\n",
    "\n",
    "PARSE GOES BELOW HERE\n",
    "```\n",
    "1\tYou\tyou\t_\tPRP\t_\t3\tnsubj\t_\t_\n",
    "2\tshould\tshould\t_\tMD\t_\t3\taux\t_\t_\n",
    "3\tcheck\tcheck\t_\tVB\t_\t0\troot\t_\t_\n",
    "4\tthe\tthe\t_\tDT\t_\t5\tdet\t_\t_\n",
    "5\tmileage\tmileage\t_\tNN\t_\t3\tdobj\t_\t_\n",
    "6\ton\ton\t_\tIN\t_\t9\tcase\t_\t_\n",
    "7\tyour\tyou\t_\tPRP$\t_\t9\tnmod:poss\t_\t_\n",
    "8\tred\tred\t_\tJJ\t_\t9\tamod\t_\t_\n",
    "9\tcar\tcar\t_\tNN\t_\t5\tnmod\t_\t_\n",
    "10\t.\t.\t_\t.\t_\t3\tpunct\t_\t_\n",
    "\n",
    "```\n",
    "PARSE GOES ABOVE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4:** Please describe your correction and why it solves the error.\n",
    "\n",
    "The adjective 'red' just qualifies the car. The presence of the word should not change the dependency link between 'mileage' and 'car'. I've arrived at the result by going through the documentation and experimenting with sentences (specifically by adding more qualifiers in order to test whether long-distance relationships between entities are working or not)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
